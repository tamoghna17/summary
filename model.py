# -*- coding: utf-8 -*-
"""summary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zcaAcHrkOoxA-wIddKz0pieGk2sLV3Bb
"""

!pip install transformers
import os
import torch
from transformers import BertTokenizer , BertModel
import numpy as np
import glob
import pickle as pkl
from google.colab import drive
drive.mount('/content/drive')

dataset_path="/content/drive/MyDrive/funny_text"
#List files in the dataset folder
files=os.listdir(dataset_path)
print(files)
# print(type(files))

data=[]
#read content of the files
for file in files:
  file_path=os.path.join(dataset_path,file)
  with open(file_path,'r',encoding='utf-8') as f:
    content=f.read()
    data.append(content)

# for i in range(10):
#   print(data[i][:1000])
#   print('\n')

type(data)

for i in range(len(data)):
  print(data[i][:100])
  print('\n')

text=" ".join(str(x) for x in data)

text

# # #tokenizing data
# # import nltk
# # from nltk.tokenize import word_tokenize
# # import tensorflow as tf
# # # from tensorflow.keras.preprocessing.text import Tokenizer
# # from transformers import GPT2LMHeadModel, GPT2Tokenizer

# # # nltk.download('punkt')

# # # tokens=word_tokenize(content) #content not data
# # # #GPT2Tokenizer to create a vocabulary
# # # tokenizer=GPT2Tokenizer()
# # # tokenizer.fit_on_texts(tokens)

# # # total_words=len(tokenizer.word_index)+1

# # tokenizer=GPT2Tokenizer.from_pretrained("gpt2")
# # model=GPT2LMHeadModel.from_pretrained("gpt2")

# # input_sequences=[]

# # max_sequence_length=512

# # for i in data:
# #   tokenized_text=tokenizer.encode(i,return_tensors="pt",add_special_tokens=False)

# #   # input_sequences.append(tokenized_text)
# #   while len(i)>max_sequence_length:
# #     chunk=i[:max_sequence_length]
# #     # tokenized_text=tokenizer.encode(i,return_tensors="pt",add_special_tokens=False)
# #     input_sequences.append(chunk)
# #     tokenized_text=tokenized_text[max_sequence_length:]

# #   input_sequences.append(tokenized_text)

# #   # tokenized_content=tokenizer.encode(i,return_tensors="pt",add_special_tokens=False)
# #   # input_sequences.append(tokenized_content)


# from transformers import GPT2Tokenizer, GPT2LMHeadModel

# tokenizer=GPT2Tokenizer.from_pretrained('gpt2')
# model=GPT2LMHeadModel.from_pretrained('gpt2')

# max_sequence_length=1084
# batch_size=8 #adjust batch size based on available memory

# input_sequences=[]
# batch=[]

# for i,text in enumerate(data):
#   tokenized_text=tokenizer.encode(text,return_tensors='pt',add_special_tokens='False')

#   while(len(tokenized_text)>max_sequence_length):
#     chunk=i[:max_sequence_length]
#     input_sequences.append(chunk)
#     tokenized_text=tokenized_text[max_sequence_length:]

#   batch.append(tokenized_text)

#   if len(batch)==batch_size or i==len(data)-1:
#     input_sequences.extend(data)
#     batch=[]

# import torch
# # for line in tokens:
# #   token_list=tokenizer.texts_to_sequences([line])[0]
# #   for i in range(1,len(token_list)):
# #     n_grams=token_list[:i+1]
# #     input_sequences.append(n_grams)

# # for f in files:
# #   tokenized_text=tokenizer.encode(content,return_tens)

# generated_text=[]
# max_new_tokens=100

# for tokenized_text in input_sequences:
#   input_ids=torch.tensor(tokenizer.convert_tokens_to_ids(tokenized_text)).unsqueeze(0)

#   with torch.no_grad():
#     output=model.generate(input_ids,max_length=len(tokenized_text)+max_new_tokens,temperature=0.1,pad_token_id=tokenizer.pad_token_id)
#   generated_text.append(tokenizer.decode(output[0],skip_special_tokens=True))
# # for input_sequence in input_sequences:
# #   output=model.generate(input_sequence,max_length=100,num_return_sequences=1,temperature=0.1)
# #   generated_text+=generated_text+tokenizer.decode(output[0],skip_special_tokens=True)

# # print(generated_text)
# for i,text in enumerate(generated_text):
#   print(text)

# len(generated_text)

# # Pad sequences for uniform length

# from keras.preprocessing.sequence import pad_sequences
# from keras.utils import to_categorical

# max_sequence_length=max([len(x) for x in input_sequences])
# input_sequences=pad_sequences(input_sequences , maxlen=max_sequence_length , padding='pre')

# #create predictors and labels
# X,y=input_sequences[:,:-1],input_sequences[:,-1]
# y=to_categorical(y,num_classes=total_words)

# # from keras.models import Sequential
# # from keras.layers import Embedding,LSTM,Dense

# # model=Sequential()
# # model.add(Embedding(total_words,100,input_length=max_sequence_length-1))
# # model.add(LSTM(100))
# # model.add(Dense(total_words,activation='softmax'))
# # model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

# model_name='gpt2'
# model=GPT2LMHeadModel.from_pretrained(model_name)
# tokenizer=GPT2Tokenizer.from_pretrained(model_name)

# model.eval()

# model.fit(X,y,epochs=75,verbose=1)

# def generate_comedy(seed_text,model,tokenizer,max_sequence_length):

#   # for i in range(next_words):
#   #   token_list=tokenizer.texts_to_sequences([seed_text])[0]
#   #   token_list=pad_sequences([token_list],maxlen=max_sequence_length-1,padding='pre')
#   #   predict_probs=model.predict(token_list,verbose=0)
#   #   predict_index=np.argmax(predict_probs)
#   #   output_word=""
#   #   for word,index in tokenizer.word_index.items():
#   #     if index==predict_index:
#   #       output_word=word
#   #       break
#   #   seed_text+=" " +output_word
#   # return seed_text

#   input_ids=tokenizer.encode(seed_text,return_tensors='pt')

#   #gpt-2 uses GPT-2 uses a technique called "nucleus sampling" (top-p sampling) to generate text
#   #This technique selects the next word from a truncated probability distribution, excluding words that fall below a certain cumulative probability threshold (top_p)
#   #This helps in preventing the model from generating overly verbose or nonsensical responses
#   #experiment with the temperature parameter, which controls the randomness of the generated text. Higher values (e.g., 1.0) increase randomness, while lower values
#   #(e.g., 0.7) make the output more deterministic.

#   output=model.generate(input_ids,max_length=max_sequence_length,num_beams=100,no_repeat_ngram_size=10,top_k=500,top_p=0.05,temperature=100,
#                         pad_token_id=tokenizer.eos_token_id,attention_mask=torch.ones(input_ids.shape))

#   comedy=tokenizer.decode(output[0],skip_special_tokens=True)

#   return comedy


# comedy=generate_comedy(" ",model,tokenizer,500)

# print(comedy)

!pip install spacy
import spacy
from spacy.lang.en.stop_words import STOP_WORDS as stopwords

stopwords=list(stopwords)
from string import punctuation
punctuation=punctuation+ '\n'

nlp = spacy.load('en_core_web_sm')
doc= nlp(text)
tokens=[token.text for token in doc]
print(tokens)

word_frequencies={}
for word in doc:
    if word.text.lower() not in stopwords:
        if word.text.lower() not in punctuation:
            if word.text not in word_frequencies.keys():
                word_frequencies[word.text] = 1
            else:
                word_frequencies[word.text] += 1

print(word_frequencies)

max_frequency=max(word_frequencies.values())
for word in word_frequencies.keys():
    word_frequencies[word]=word_frequencies[word]/max_frequency

print(word_frequencies)

sentence_tokens= [sent for sent in doc.sents]
print(sentence_tokens)

sentence_scores = {}
for sent in sentence_tokens:
    for word in sent:
        if word.text.lower() in word_frequencies.keys():
            if sent not in sentence_scores.keys():
             sentence_scores[sent]=word_frequencies[word.text.lower()]
            else:
             sentence_scores[sent]+=word_frequencies[word.text.lower()]

sentence_scores

len(sentence_tokens)

from heapq import nlargest
select_length=int(len(sentence_tokens)*0.3)
select_length
summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)
summary

final_summary=[word.text for word in summary]
final_summary
summary=''.join(final_summary)
summary

